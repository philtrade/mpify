{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T17:08:25.188012Z",
     "start_time": "2020-07-18T17:08:25.171594Z"
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '__main__.mpify'; '__main__' is not a package",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3b8ccf1541d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmpify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mranch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_torchddp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '__main__.mpify'; '__main__' is not a package"
     ]
    }
   ],
   "source": [
    "from mpify import ranch, in_torchddp\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "\"\"\"\n",
    "    Applying mpify to the examples in PyTorch Distributed tutorial at\n",
    "    https://pytorch.org/tutorials/intermediate/dist_tuto.html\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Blocking point-to-point communication.\"\"\"\n",
    "def run_blocking(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    if rank == 0:\n",
    "        for r in range(1,size):\n",
    "            tensor += 1\n",
    "            # Send the tensor to process r\n",
    "            dist.send(tensor=tensor, dst=r)\n",
    "            print(f'Rank 0 started blocking send to rank {r}', flush=True)\n",
    "    else:\n",
    "        # Receive tensor from process 0\n",
    "        dist.recv(tensor=tensor, src=0)\n",
    "    print('Rank ', rank, ' has data ', tensor[0], flush=True)\n",
    "\n",
    "\"\"\"Non-blocking point-to-point communication.\"\"\"\n",
    "def run_nonblocking(rank, size):\n",
    "    print(f\"Rank {rank}, name: {__name__}\")\n",
    "    tensor = torch.zeros(1)\n",
    "    req = None\n",
    "    if rank == 0:\n",
    "        for r in range(1,size):\n",
    "            tensor = tensor+1\n",
    "            # Send the tensor to process r\n",
    "            req = dist.isend(tensor=tensor, dst=r)\n",
    "            print(f'Rank 0 started nonblocking send to rank {r}', flush=True)\n",
    "            req.wait() # Must call req.wait() before next iteration, to avoid data corruption\n",
    "    else:\n",
    "        # Receive tensor from process 0\n",
    "        req = dist.irecv(tensor=tensor, src=0)\n",
    "        print(f'Rank {rank} started nonblocking receive', flush=True)\n",
    "        req.wait()\n",
    "    print('Rank ', rank, ' has data ', tensor[0], flush=True)\n",
    "\n",
    "\"\"\" All-Reduce example.\"\"\"\n",
    "def run_allreduce(rank, size):\n",
    "    \"\"\" Simple point-to-point communication. \"\"\"\n",
    "    tensor = torch.ones(1)\n",
    "    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n",
    "    print('AllReduce: Rank ', rank, ' has data ', tensor[0], flush=True)\n",
    "\n",
    "def rank_size_wrapper(fn):\n",
    "    def new_fn(*args, **kwargs):\n",
    "        import os\n",
    "        return fn(int(os.environ.get('RANK')), int(os.environ.get('WORLD_SIZE')), *args, **kwargs)\n",
    "    return new_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T17:08:40.501986Z",
     "start_time": "2020-07-18T17:08:39.133018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing env to contextualize(): {}\n",
      "Rank 0, name: __main__\n",
      "Rank 0 started nonblocking send to rank 1\n",
      "Rank 0 started nonblocking send to rank 2\n",
      "Rank 0 started nonblocking send to rank 3\n",
      "Rank 0 started nonblocking send to rank 4\n",
      "Rank  0  has data  tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "size = torch.cuda.device_count() if torch.cuda.is_available() else 5\n",
    "    \n",
    "imports='''\n",
    "from mpify import in_torchddp, ddp_rank, ddp_worldsize\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "'''\n",
    "\n",
    "in_torchddp(size, rank_size_wrapper(run_nonblocking), imports=imports)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}