{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `mpify` to Train Fastai2/Course-V4 Examples \"Distributedly\" on Multiple GPUs\n",
    "\n",
    "###  To train a `fastai2` learner on multiple processes inside a Jupyter notebook\n",
    "1. Ensure each process has its own copy of: the model, the access to its GPU, and dataloader.\n",
    "\n",
    "    * The `DataLoader` must be re-created fresh on each process, because the CUDA GPU context it might initialize cannot be reused in another process.\n",
    "    CUDA PyTorch tensors created in the parent Jupyter process should not be passed to the subprocesses, otherwise it will incur 600MB memory, *per subprocess*, on the original GPU associated with the tensor.\n",
    "\n",
    "    * For other variables (`path` of untar'ed dataset, or `df` a loaded DataFrame), or the many helper functions, they can be passed to the distributed training API `in_torchddp()` via `imports=` and `need=` parameters.\n",
    "\n",
    "2. In each process `from fastai2.distributed import *`, and surround the fitting function `with learn.distrib_ctx()`.\n",
    "\n",
    "### Quick links to course-v4 chapters `mpify`-ed:\n",
    "\n",
    "[01_intro.ipynb](/examples/fastai2_course-v4_01_intro_distrib.ipynb)\n",
    "\n",
    "[05_pet_breeds.ipynb](/examples/fastai2_course-v4_05_pet_breeds_distrib.ipynb)\n",
    "\n",
    "[06_multicat.ipynb](/examples/fastai2_course-v4_06_multicat_distrib.ipynb)\n",
    "\n",
    "[07_sizing_and_tta.ipynb](/examples/fastai2_course-v4_07_sizing_tta_distrib.ipynb)\n",
    "\n",
    "[08_collab.ipynb](/examples/fastai2_course-v4_08_collab_distrib.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are distributed training of examples correspond to fastai2 course-v4 <a href='https://github.com/fastai/course-v4/blob/master/nbs/01_intro.ipynb' target='_blank'>`01_intro.ipynb`</a>\n",
    "\n",
    "\n",
    "### <a name=\"01intro\"></a> 01_intro.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to train cnn on multiple GPUs\n",
    "from mpify import *\n",
    "\n",
    "imports='''from utils import *\n",
    "from fastai2.vision.all import *\n",
    "from fastai2.distributed import *\n",
    "'''\n",
    "\n",
    "from fastai2.vision.all import *\n",
    "path = untar_data(URLs.PETS)/'images'\n",
    "\n",
    "def is_cat(x): return x[0].isupper()\n",
    "    \n",
    "def train_cnn():\n",
    "\n",
    "    dls = ImageDataLoaders.from_name_func(\n",
    "        path, get_image_files(path), valid_pct=0.2, seed=42,\n",
    "        label_func=is_cat, item_tfms=Resize(224))\n",
    "\n",
    "    learn = cnn_learner(dls, resnet34, metrics=error_rate)\n",
    "    with learn.distrib_ctx():\n",
    "        learn.fine_tune(4)\n",
    "    \n",
    "    return learn\n",
    "\n",
    "ngpus = 3\n",
    "\n",
    "learn = in_torchddp(ngpus, train_cnn, imports=imports, need=\"path is_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to train unet on multiple GPUs\n",
    "from mpify import in_torchddp\n",
    "\n",
    "imports='''from utils import *\n",
    "from fastai2.vision.all import *\n",
    "from fastai2.distributed import *\n",
    "'''\n",
    "\n",
    "from fastai2.vision.all import *\n",
    "    \n",
    "def train_unet():\n",
    "    path = untar_data(URLs.CAMVID_TINY)\n",
    "    dls = SegmentationDataLoaders.from_label_func(\n",
    "        path, bs=8, fnames = get_image_files(path/\"images\"),\n",
    "        label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n",
    "        codes = np.loadtxt(path/'codes.txt', dtype=str)\n",
    "    )\n",
    "\n",
    "    learn = unet_learner(dls, resnet34)\n",
    "    with learn.distrib_ctx(): learn.fine_tune(20)\n",
    "    return learn\n",
    "\n",
    "ngpus = 3\n",
    "learn = in_torchddp(ngpus, train_unet, imports=imports)\n",
    "learn.show_results(max_n=6, figsize=(7,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train text classifier on multiple GPUs\n",
    "\n",
    "from mpify import in_torchddp\n",
    "\n",
    "imports='''from utils import *\n",
    "from fastai2.text.all import *\n",
    "from fastai2.distributed import *\n",
    "'''\n",
    "\n",
    "def train_imdb_classifier():\n",
    "    \n",
    "    dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test', bs=96)\n",
    "    learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
    "    \n",
    "    import os\n",
    "    if int(os.environ['WORLD_SIZE']) > 1 and torch.__version__.startswith(\"1.4\"): DistributedTrainer.fup = True\n",
    "        \n",
    "    with learn.distrib_ctx(): learn.fine_tune(4, 1e-2)\n",
    "    return learn\n",
    "\n",
    "# To train on 3 GPUs with distributed data parallel\n",
    "learn = in_torchddp(3, train_imdb_classifier, imports=imports)\n",
    "\n",
    "learn.predict(\"I really liked that movie!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train tabular in multiple GPUs\n",
    "\n",
    "from mpify import in_torchddp\n",
    "\n",
    "imports='''from utils import *\n",
    "from fastai2.tabular.all import *\n",
    "from fastai2.distributed import *\n",
    "'''\n",
    "\n",
    "def train_tabular():\n",
    "    path = untar_data(URLs.ADULT_SAMPLE)\n",
    "\n",
    "    dls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names=\"salary\",\n",
    "        cat_names = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                     'relationship', 'race'],\n",
    "        cont_names = ['age', 'fnlwgt', 'education-num'],\n",
    "        procs = [Categorify, FillMissing, Normalize])\n",
    "\n",
    "    learn = tabular_learner(dls, metrics=accuracy)\n",
    "    with learn.distrib_ctx():\n",
    "        learn.fit_one_cycle(3)\n",
    "    return learn\n",
    "\n",
    "# To train on 3 GPUs with distributed data parallel\n",
    "learn = in_torchddp(3, train_tabular, imports=imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train collab in multiple GPUs\n",
    "\n",
    "from mpify import in_torchddp\n",
    "\n",
    "imports='''from utils import *\n",
    "from fastai2.collab import *\n",
    "from fastai2.distributed import *\n",
    "'''\n",
    "\n",
    "def train_collab():\n",
    "    path = untar_data(URLs.ML_SAMPLE)\n",
    "    dls = CollabDataLoaders.from_csv(path/'ratings.csv')\n",
    "    learn = collab_learner(dls, y_range=(0.5,5.5))\n",
    "    with learn.distrib_ctx():\n",
    "        learn.fine_tune(40)\n",
    "    return learn\n",
    "\n",
    "# To train on 3 GPUs with distributed data parallel\n",
    "learn = in_torchddp(3, train_collab, imports=imports)\n",
    "learn.show_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}