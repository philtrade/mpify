{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `mpify` to Train Fastai2/Course-V4 Examples \"Distributedly\" on Multiple GPUs\n",
    "\n",
    "###  To train a `fastai2` learner on multiple processes inside a Jupyter notebook\n",
    "\n",
    "The key is to ensure each process has its own copy of: the model, the access to its GPU, and dataloader.\n",
    "\n",
    "The `DataLoader` must be re-created fresh on each process, because the CUDA GPU context it might initialize cannot be reused in another process.\n",
    "CUDA PyTorch tensors created in the parent Jupyter process should not be passed to the subprocesses, otherwise it will incur 600MB memory, *per subprocess*, on the original GPU associated with the tensor.\n",
    "\n",
    "For other variables (`path` of untar'ed dataset, or `df` a loaded DataFrame), or the many helper functions, they can be passed to the distributed training API `in_torchddp()` via `imports=` and `need=` parameters.\n",
    "\n",
    "Links to `mpify'-ed 'Fastai2' course-v4 notebooks examples:\n",
    "\n",
    "[01_intro.ipynb](/examples/fastai2_course-v4_01_intro_distrib.ipynb)\n",
    "\n",
    "[05_pet_breeds.ipynb](/examples/fastai2_course-v4_05_pet_breeds_distrib.ipynb)\n",
    "\n",
    "[06_multicat.ipynb](/examples/fastai2_course-v4_06_multicat_distrib.ipynb)\n",
    "\n",
    "[07_sizing_and_tta.ipynb](/examples/fastai2_course-v4_07_sizing_tta_distrib.ipynb)\n",
    "\n",
    "[08_collab.ipynb](/examples/fastai2_course-v4_08_collab_distrib.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are distributed training of examples correspond to fastai2 course-v4 <a href='https://github.com/fastai/course-v4/blob/master/nbs/08_collab.ipynb' target='_blank'>`08_collab.ipynb`</a>\n",
    "\n",
    "### <a name='08collab'></a> 08 Collab\n",
    "\n",
    "In the first FIVE (5) training examples in notebook 08_collab, several variables depend on `dls` --- we move them to inside the target function after the dataloader `dls` is created.\n",
    "\n",
    "Those created before `dls`, we keep them at the global scope, and pass them in via `need=`\n",
    "\n",
    "And since 3 versions of `DotProduct` classes are used, we parameterize it as `dp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from fastai2.collab import *\n",
    "from fastai2.tabular.all import *\n",
    "\n",
    "path = untar_data(URLs.ML_100k)\n",
    "ratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None,\n",
    "                      names=['user','movie','rating','timestamp'])\n",
    "\n",
    "movies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1',\n",
    "                     usecols=(0,1), names=('movie','title'), header=None)\n",
    "\n",
    "ratings = ratings.merge(movies)\n",
    "\n",
    "class DotProduct(Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors):\n",
    "        self.user_factors = Embedding(n_users, n_factors)\n",
    "        self.movie_factors = Embedding(n_movies, n_factors)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users = self.user_factors(x[:,0])\n",
    "        movies = self.movie_factors(x[:,1])\n",
    "        return (users * movies).sum(dim=1)\n",
    "\n",
    "def train_dotproduct(nepochs, *args, load:str=None, dp=DotProduct, **kwargs):\n",
    "\n",
    "    dls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\n",
    "    \n",
    "    n_users  = len(dls.classes['user'])\n",
    "    n_movies = len(dls.classes['title'])\n",
    "    n_factors = 5\n",
    "    user_factors = torch.randn(n_users, n_factors)\n",
    "    movie_factors = torch.randn(n_movies, n_factors)\n",
    "\n",
    "    model = dp(n_users, n_movies, 50)\n",
    "    learn = Learner(dls, model, loss_func=MSELossFlat())\n",
    "    \n",
    "    if load: learn.load(load); print(f'Model and state loaded from {load}')\n",
    "\n",
    "    with learn.distrib_ctx():\n",
    "        learn.fit_one_cycle(nepochs, *args, **kwargs)\n",
    "    return learn\n",
    "\n",
    "from mpify import in_torchddp\n",
    "ngpus = 3\n",
    "\n",
    "imports='''\n",
    "from utils import *\n",
    "from fastai2.collab import *\n",
    "from fastai2.tabular.all import *\n",
    "from fastai2.distributed import *\n",
    "'''\n",
    "\n",
    "need=\"path ratings\"\n",
    "\n",
    "learn = in_torchddp(ngpus, train_dotproduct, 5, 5e-3, dp=DotProduct, imports=imports, need=need)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the notebook modified the `DotProduct` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new dotproduct class\n",
    "class DotProduct(Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n",
    "        self.user_factors = Embedding(n_users, n_factors)\n",
    "        self.movie_factors = Embedding(n_movies, n_factors)\n",
    "        self.y_range = y_range\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users = self.user_factors(x[:,0])\n",
    "        movies = self.movie_factors(x[:,1])\n",
    "        return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n",
    "\n",
    "learn = in_torchddp(ngpus, train_dotproduct, 5, 5e-3, dp=DotProduct, imports=imports, need=need)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new variant: `DotProductBias`, we update `dp` in `in_torchddp(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductBias(Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n",
    "        self.user_factors = Embedding(n_users, n_factors)\n",
    "        self.user_bias = Embedding(n_users, 1)\n",
    "        self.movie_factors = Embedding(n_movies, n_factors)\n",
    "        self.movie_bias = Embedding(n_movies, 1)\n",
    "        self.y_range = y_range\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users = self.user_factors(x[:,0])\n",
    "        movies = self.movie_factors(x[:,1])\n",
    "        res = (users * movies).sum(dim=1, keepdim=True)\n",
    "        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n",
    "        return sigmoid_range(res, *self.y_range)\n",
    "    \n",
    "learn = in_torchddp(ngpus, train_dotproduct, 5, 5e-3, dp=DotProductBias, imports=imports, need=need)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08 Collab . Weight Decay, and Create Our Own Embedding Module\n",
    "\n",
    "The chapter proceeds to adding weight decay to the training loop --- `**kwargs` in `in_torchddp()` will pick it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = in_torchddp(ngpus, train_dotproduct, 5, 5e-3, wd=0.1, dp=DotProductBias, imports=imports, need=need)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further customize a `DotProductBias` class to use a new local function `create_params`?  Add it to `need=`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Our Own Embedding Module\n",
    "def create_params(size):\n",
    "    return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))\n",
    "\n",
    "class DotProductBias(Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n",
    "        self.user_factors = create_params([n_users, n_factors])\n",
    "        self.user_bias = create_params([n_users])\n",
    "        self.movie_factors = create_params([n_movies, n_factors])\n",
    "        self.movie_bias = create_params([n_movies])\n",
    "        self.y_range = y_range\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users = self.user_factors[x[:,0]]\n",
    "        movies = self.movie_factors[x[:,1]]\n",
    "        res = (users*movies).sum(dim=1)\n",
    "        res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n",
    "        return sigmoid_range(res, *self.y_range)\n",
    "\n",
    "need = f\"{need} create_params\"  # add create_params\n",
    "learn = in_torchddp(ngpus, train_dotproduct, 5, 5e-3, wd=0.1, dp=DotProductBias, imports=imports, need=need)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tedious steps are all abstracted away in `collab_learner`, see how much shorter the new training function `train_collab()` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_collab(nepochs, *args, load:str=None, **kwargs):\n",
    "    dls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\n",
    "\n",
    "    learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\n",
    "    \n",
    "    if load: learn.load(load); print(f'Model and state loaded from {load}')\n",
    "\n",
    "    with learn.distrib_ctx(): learn.fit_one_cycle(nepochs, *args)\n",
    "    return learn\n",
    "\n",
    "learn = in_torchddp(ngpus, train_collab, 5, 5e-3, wd=0.1, imports=imports, need=need)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08 Collab . Boostraping Collab Model\n",
    "\n",
    "The noteboook then builds the model in two ways.\n",
    "\n",
    "1. Build a PyTorch model, using the embeddings from the dataloader, pass that to Learner.  We have to add `embs` and `CollabNN` to `need=`.\n",
    "\n",
    "or\n",
    " 2. Let `collab_learner(.., use_nn=True, y_range=.., layers=[..])` takes care of the same details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T19:35:25.701175Z",
     "start_time": "2020-07-19T19:35:25.691498Z"
    }
   },
   "outputs": [],
   "source": [
    "embs = get_emb_sz(learn.dls)\n",
    "\n",
    "class CollabNN(Module):\n",
    "    def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):\n",
    "        self.user_factors = Embedding(*user_sz)\n",
    "        self.item_factors = Embedding(*item_sz)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(user_sz[1]+item_sz[1], n_act),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_act, 1))\n",
    "        self.y_range = y_range\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embs = self.user_factors(x[:,0]),self.item_factors(x[:,1])\n",
    "        x = self.layers(torch.cat(embs, dim=1))\n",
    "        return sigmoid_range(x, *self.y_range)\n",
    "\n",
    "def train_bootstrap(nepochs, *args, load:str=None, **kwargs):\n",
    "    dls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\n",
    "\n",
    "    model = CollabNN(*embs)\n",
    "    \n",
    "    learn = Learner(dls, model, loss_func=MSELossFlat())    \n",
    "    if load: learn.load(load); print(f'Model and state loaded from {load}')\n",
    "\n",
    "    with learn.distrib_ctx(): learn.fit_one_cycle(nepochs, *args)\n",
    "    return learn\n",
    "\n",
    "def train_collab_learner(nepochs, *args, load:str=None, **kwargs):\n",
    "    dls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\n",
    "    \n",
    "    learn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50])    \n",
    "    if load: learn.load(load); print(f'Model and state loaded from {load}')\n",
    "\n",
    "    with learn.distrib_ctx(): learn.fit_one_cycle(nepochs, *args)\n",
    "    return learn\n",
    "\n",
    "need='ratings CollabNN embs'\n",
    "learn = in_torchddp(ngpus, train_bootstrap, 5, 5e-3, wd=0.1, imports=imports, need=need)\n",
    "\n",
    "learn = in_torchddp(ngpus, train_collab_learner, 5, 5e-3, wd=0.1, imports=imports, need=need)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}