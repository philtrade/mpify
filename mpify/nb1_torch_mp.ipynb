{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T18:15:09.012226Z",
     "start_time": "2020-07-17T18:15:08.995510Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpify import in_torchddp, ddp_rank, ddp_worldsize\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "\"\"\"\n",
    "    Applying mpify to the examples in PyTorch Distributed tutorial at\n",
    "    https://pytorch.org/tutorials/intermediate/dist_tuto.html\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Blocking point-to-point communication.\"\"\"\n",
    "def run_blocking(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    if rank == 0:\n",
    "        for r in range(1,size):\n",
    "            tensor += 1\n",
    "            # Send the tensor to process r\n",
    "            dist.send(tensor=tensor, dst=r)\n",
    "            print(f'Rank 0 started blocking send to rank {r}', flush=True)\n",
    "    else:\n",
    "        # Receive tensor from process 0\n",
    "        dist.recv(tensor=tensor, src=0)\n",
    "    print('Rank ', rank, ' has data ', tensor[0], flush=True)\n",
    "\n",
    "\"\"\"Non-blocking point-to-point communication.\"\"\"\n",
    "def run_nonblocking(rank, size):\n",
    "    print(f\"Rank {rank}, name: {__name__}\")\n",
    "    # import torch\n",
    "    # import torch.distributed as dist\n",
    "    tensor = torch.zeros(1)\n",
    "    req = None\n",
    "    if rank == 0:\n",
    "        for r in range(1,size):\n",
    "            tensor = tensor+1\n",
    "            # Send the tensor to process r\n",
    "            req = dist.isend(tensor=tensor, dst=r)\n",
    "            print(f'Rank 0 started nonblocking send to rank {r}', flush=True)\n",
    "            req.wait() # Must call req.wait() before next iteration, to avoid data corruption\n",
    "    else:\n",
    "        # Receive tensor from process 0\n",
    "        req = dist.irecv(tensor=tensor, src=0)\n",
    "        print(f'Rank {rank} started nonblocking receive', flush=True)\n",
    "        req.wait()\n",
    "    print('Rank ', rank, ' has data ', tensor[0], flush=True)\n",
    "\n",
    "\"\"\" All-Reduce example.\"\"\"\n",
    "def run_allreduce(rank, size):\n",
    "    \"\"\" Simple point-to-point communication. \"\"\"\n",
    "    # group = dist.new_group([0, 1])\n",
    "    tensor = torch.ones(1)\n",
    "    dist.all_reduce(tensor, op=dist.ReduceOp.SUM) #, group=group)\n",
    "    print('AllReduce: Rank ', rank, ' has data ', tensor[0], flush=True)\n",
    "\n",
    "def runner(fn):\n",
    "    return fn(ddp_rank(), ddp_worldsize())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T18:15:14.824662Z",
     "start_time": "2020-07-17T18:15:13.529088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing env to contextualize(): {}\n",
      "Rank 0, name: __main__\n",
      "Rank 0 started nonblocking send to rank 1\n",
      "Rank 0 started nonblocking send to rank 2\n",
      "Rank 0 started nonblocking send to rank 3\n",
      "Rank 0 started nonblocking send to rank 4\n",
      "Rank  0  has data  tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "size = 5 # torch.cuda.device_count() if torch.cuda.is_available() else 5\n",
    "    \n",
    "imports='''\n",
    "from mpify import in_torchddp, ddp_rank, ddp_worldsize\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "'''\n",
    "\n",
    "# ideal:\n",
    "#   mpify.env(imports=['torch', 'numpy as np', 'from x import y as z'], need=\"obja objB objC funcA\")\n",
    "#   ranch(size, run_nonblocking, imports=[import_specs], need={} )\n",
    "in_torchddp(size, runner, run_nonblocking, imports=imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T08:00:11.998004Z",
     "start_time": "2020-07-17T08:00:11.993744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runner'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def foo(*args, **kwargs):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
